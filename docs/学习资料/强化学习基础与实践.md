# 强化学习基础与实践

## 一、强化学习简介

### 1. 什么是强化学习？
想象你在教一只小狗玩接飞盘：
- 接住飞盘时给它奖励（零食）
- 没接住时不给奖励
- 小狗通过尝试和奖励学会接飞盘

这就是强化学习的基本思想：通过"试错"和"奖励"来学习最优策略。

### 2. 核心概念
- 智能体（Agent）：做决策的主体
- 环境（Environment）：智能体所在的世界
- 状态（State）：环境的当前情况
- 动作（Action）：智能体可以采取的行为
- 奖励（Reward）：环境对动作的反馈

## 二、基础算法详解

### 1. Q-Learning
就像制作一张"行动指南表"：
```python
import numpy as np

class QLearning:
    def __init__(self, n_states, n_actions, learning_rate=0.1, gamma=0.95):
        self.q_table = np.zeros((n_states, n_actions))
        self.lr = learning_rate  # 学习率
        self.gamma = gamma      # 折扣因子
        
    def choose_action(self, state, epsilon=0.1):
        # ε-贪婪策略
        if np.random.random() < epsilon:
            return np.random.randint(self.q_table.shape[1])
        return np.argmax(self.q_table[state])
    
    def learn(self, state, action, reward, next_state):
        # Q值更新
        old_value = self.q_table[state, action]
        next_max = np.max(self.q_table[next_state])
        
        # Q-learning公式
        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * next_max)
        self.q_table[state, action] = new_value
```

### 2. DQN（深度Q网络）
将Q表格升级为神经网络：
```python
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_size)
        )
    
    def forward(self, x):
        return self.network(x)

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.policy_net = DQN(state_size, action_size)
        self.target_net = DQN(state_size, action_size)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        
        self.optimizer = torch.optim.Adam(self.policy_net.parameters())
        self.memory = ReplayMemory(10000)
    
    def select_action(self, state, epsilon=0.1):
        if np.random.random() < epsilon:
            return torch.tensor([[np.random.randint(self.n_actions)]])
        
        with torch.no_grad():
            return self.policy_net(state).max(1)[1].view(1, 1)
```

## 三、进阶算法

### 1. 策略梯度（Policy Gradient）
直接学习最优策略：
```python
class PolicyGradient(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(PolicyGradient, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, x):
        return self.network(x)
    
    def get_action(self, state):
        probs = self.forward(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action, action_dist.log_prob(action)
```

### 2. Actor-Critic
演员负责行动，评论家负责评价：
```python
class ActorCritic(nn.Module):
    def __init__(self, input_size, n_actions):
        super(ActorCritic, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, n_actions),
            nn.Softmax(dim=-1)
        )
        
        self.critic = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x):
        value = self.critic(x)
        policy_dist = self.actor(x)
        return value, policy_dist
```

## 四、实践应用

### 1. 游戏AI
以贪吃蛇为例：
```python
class SnakeGame:
    def __init__(self, size=10):
        self.size = size
        self.reset()
    
    def get_state(self):
        # 获取游戏状态
        state = np.zeros((4,))  # [食物相对位置x,y, 蛇头方向x,y]
        state[0] = (self.food[0] - self.snake[0][0]) / self.size
        state[1] = (self.food[1] - self.snake[0][1]) / self.size
        state[2:] = self.direction
        return state
    
    def step(self, action):
        # 执行动作
        old_distance = self._get_food_distance()
        
        # 更新蛇的位置
        self._update_position(action)
        
        # 计算奖励
        reward = self._calculate_reward(old_distance)
        
        # 检查游戏是否结束
        done = self._is_game_over()
        
        return self.get_state(), reward, done
```

### 2. 机器人控制
机械臂抓取示例：
```python
class RobotArm:
    def __init__(self):
        self.joints = np.zeros(6)  # 6个关节
        self.target = None
    
    def get_state(self):
        return np.concatenate([self.joints, self.target])
    
    def step(self, action):
        # 执行关节动作
        self.joints += action
        
        # 计算末端执行器位置
        end_effector_pos = self._forward_kinematics(self.joints)
        
        # 计算奖励（基于距离目标的远近）
        distance = np.linalg.norm(end_effector_pos - self.target)
        reward = -distance
        
        # 判断是否完成任务
        done = distance < 0.01
        
        return self.get_state(), reward, done
```

## 五、高级技巧

### 1. 经验回放
提高样本利用效率：
```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return state, action, reward, next_state, done
```

### 2. 优先经验回放
重要的经验多学习：
```python
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6):
        self.capacity = capacity
        self.alpha = alpha
        self.buffer = []
        self.priorities = np.zeros(capacity)
    
    def push(self, state, action, reward, next_state, done):
        max_priority = np.max(self.priorities) if self.buffer else 1.0
        
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.position] = (state, action, reward, next_state, done)
        
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity
```

## 六、常见问题与解决方案

### 1. 探索与利用
平衡尝试新动作和使用已知好动作：
```python
def epsilon_greedy(q_values, epsilon):
    if np.random.random() < epsilon:
        return np.random.randint(len(q_values))  # 探索
    return np.argmax(q_values)  # 利用
```

### 2. 奖励稀疏问题
设计合适的奖励函数：
```python
def shaped_reward(state, action, next_state):
    # 基础奖励
    base_reward = -1  # 每步的基础惩罚
    
    # 任务完成奖励
    if is_goal_reached(next_state):
        base_reward += 100
    
    # 形状奖励：鼓励向目标靠近
    distance_reward = get_distance_reward(state, next_state)
    
    return base_reward + distance_reward
```

## 七、项目实战

### 1. 自动驾驶
```python
class SelfDrivingCar:
    def __init__(self):
        self.state_size = 10  # 传感器数据
        self.action_size = 3  # 转向、加速、刹车
        
        self.agent = DQNAgent(self.state_size, self.action_size)
    
    def train(self, episodes=1000):
        for episode in range(episodes):
            state = self.reset()
            total_reward = 0
            
            while True:
                action = self.agent.select_action(state)
                next_state, reward, done = self.step(action)
                
                self.agent.memory.push(state, action, reward, next_state, done)
                self.agent.learn()
                
                state = next_state
                total_reward += reward
                
                if done:
                    break
            
            print(f"Episode {episode}, Total Reward: {total_reward}")
```

### 2. 智能交易
```python
class TradingAgent:
    def __init__(self, data):
        self.data = data
        self.state_size = 10  # 技术指标
        self.action_size = 3  # 买入、卖出、持有
        
        self.agent = ActorCritic(self.state_size, self.action_size)
    
    def get_state(self, t):
        # 构建状态向量
        window = self.data[t-10:t]
        return np.array([
            window['close'].pct_change(),
            window['volume'].pct_change(),
            window['rsi'],
            window['macd'],
        ])
    
    def step(self, action, t):
        # 执行交易
        next_price = self.data.iloc[t+1]['close']
        current_price = self.data.iloc[t]['close']
        
        # 计算收益
        reward = self._calculate_reward(action, current_price, next_price)
        
        return self.get_state(t+1), reward
```

## 八、前沿发展

### 1. 多智能体强化学习
```python
class MultiAgentEnv:
    def __init__(self, n_agents):
        self.agents = [DQNAgent() for _ in range(n_agents)]
        
    def step(self, actions):
        # 所有智能体同时行动
        next_states = []
        rewards = []
        
        for agent_id, action in enumerate(actions):
            next_state, reward = self._agent_step(agent_id, action)
            next_states.append(next_state)
            rewards.append(reward)
        
        return next_states, rewards
```

### 2. 分层强化学习
```python
class HierarchicalAgent:
    def __init__(self):
        self.meta_controller = DQNAgent()  # 高层策略
        self.controllers = [DQNAgent() for _ in range(n_subtasks)]  # 低层策略
    
    def select_action(self, state):
        # 高层选择子任务
        subtask = self.meta_controller.select_action(state)
        
        # 低层执行子任务
        return self.controllers[subtask].select_action(state)
```

## 九、学习建议

### 1. 学习路径
1. 掌握基础概念
2. 实现简单算法
3. 理解进阶方法
4. 实践实际项目

### 2. 常见陷阱
- 过度依赖深度学习
- 忽视环境设计
- 奖励函数设计不当
- 参数调节不当

### 3. 资源推荐
1. **书籍**
   - 《强化学习导论》
   - 《深度强化学习实践》

2. **课程**
   - David Silver的强化学习课程
   - Berkeley CS285

3. **代码库**
   - OpenAI Gym
   - Stable Baselines3
   - RLlib 
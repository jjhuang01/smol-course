# AIå®æˆ˜é¡¹ç›®æŒ‡å—

## ä¸€ã€é¡¹ç›®è§„åˆ’ä¸å‡†å¤‡
### 1. é¡¹ç›®é€‰æ‹©
- ğŸ¯ éš¾åº¦é€’è¿›ï¼šä»ç®€å•åˆ°å¤æ‚
- ğŸŒŸ é¢†åŸŸé€‰æ‹©ï¼šCVã€NLPã€RLç­‰
- ğŸ“Š è¯„ä¼°æ ‡å‡†ï¼š
  - æŠ€æœ¯å¯è¡Œæ€§
  - æ•°æ®å¯è·å¾—æ€§
  - è®¡ç®—èµ„æºéœ€æ±‚

### 2. ç¯å¢ƒé…ç½®
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv env
source env/bin/activate  # Linux/Mac
env\Scripts\activate     # Windows

# å®‰è£…åŸºç¡€åŒ…
pip install torch torchvision
pip install transformers datasets
pip install pandas numpy matplotlib
```

## äºŒã€å®æˆ˜é¡¹ç›®ç¤ºä¾‹
### 1. å›¾åƒåˆ†ç±»å™¨
```python
import torch
import torchvision
from torch import nn
from torchvision import transforms

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# åŠ è½½æ•°æ®
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                       download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,
                                        shuffle=True, num_workers=2)

# æ¨¡å‹å®šä¹‰
model = torchvision.models.resnet18(pretrained=True)
model.fc = nn.Linear(512, 10)  # ä¿®æ”¹æœ€åä¸€å±‚

# è®­ç»ƒå¾ªç¯
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

for epoch in range(10):
    for inputs, labels in trainloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

### 2. æ–‡æœ¬åˆ†ç±»å™¨
```python
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset

# æ•°æ®å‡†å¤‡
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.encodings = tokenizer(texts, truncation=True, padding=True)
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# æ¨¡å‹è®­ç»ƒ
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('bert-base-chinese')

# è®­ç»ƒå¾ªç¯
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

for epoch in range(3):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

### 3. å›¾åƒç”Ÿæˆå™¨
```python
# ç®€å•çš„GANå®ç°
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.Linear(256, 784),
            nn.Tanh()
        )
    
    def forward(self, x):
        return self.main(x)

# è®­ç»ƒè¿‡ç¨‹
def train_gan(generator, discriminator, dataloader):
    g_optimizer = torch.optim.Adam(generator.parameters())
    d_optimizer = torch.optim.Adam(discriminator.parameters())
    
    for epoch in range(100):
        for real_images in dataloader:
            # è®­ç»ƒåˆ¤åˆ«å™¨
            d_optimizer.zero_grad()
            batch_size = real_images.size(0)
            label_real = torch.ones(batch_size, 1)
            label_fake = torch.zeros(batch_size, 1)
            
            d_loss_real = criterion(discriminator(real_images), label_real)
            
            noise = torch.randn(batch_size, 100)
            fake_images = generator(noise)
            d_loss_fake = criterion(discriminator(fake_images.detach()), label_fake)
            
            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            d_optimizer.step()
            
            # è®­ç»ƒç”Ÿæˆå™¨
            g_optimizer.zero_grad()
            g_loss = criterion(discriminator(fake_images), label_real)
            g_loss.backward()
            g_optimizer.step()
```

## ä¸‰ã€é¡¹ç›®ä¼˜åŒ–æŠ€å·§
### 1. æ€§èƒ½ä¼˜åŒ–
- ğŸ”§ æ•°æ®åŠ è½½ä¼˜åŒ–
  - ä½¿ç”¨ `num_workers`
  - é€‚å½“çš„ `batch_size`
  - æ•°æ®é¢„å–

- ğŸš€ æ¨¡å‹åŠ é€Ÿ
```python
# ä½¿ç”¨GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 2. æ¨¡å‹è°ƒä¼˜
- ğŸ“ˆ å­¦ä¹ ç‡è°ƒæ•´
```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.1, patience=10
)
```

- ğŸ¯ æ­£åˆ™åŒ–æŠ€æœ¯
```python
# æ·»åŠ æƒé‡è¡°å‡
optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5)

# Dropoutå±‚
self.dropout = nn.Dropout(0.5)
```

## å››ã€éƒ¨ç½²ä¸æœåŠ¡åŒ–
### 1. æ¨¡å‹å¯¼å‡º
```python
# ä¿å­˜æ¨¡å‹
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
}, 'checkpoint.pth')

# åŠ è½½æ¨¡å‹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
```

### 2. WebæœåŠ¡éƒ¨ç½²
```python
from flask import Flask, request, jsonify
app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    # é¢„å¤„ç†
    input_tensor = preprocess(data)
    # æ¨ç†
    with torch.no_grad():
        output = model(input_tensor)
    # åå¤„ç†
    result = postprocess(output)
    return jsonify({'prediction': result})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## äº”ã€é¡¹ç›®ç®¡ç†æœ€ä½³å®è·µ
1. ä»£ç ç»„ç»‡
   ```
   project/
   â”œâ”€â”€ data/
   â”œâ”€â”€ models/
   â”œâ”€â”€ config/
   â”œâ”€â”€ utils/
   â”œâ”€â”€ train.py
   â”œâ”€â”€ evaluate.py
   â””â”€â”€ README.md
   ```

2. å®éªŒè®°å½•
   - ä½¿ç”¨ MLflow æˆ– Weights & Biases
   - è®°å½•è¶…å‚æ•°å’Œç»“æœ
   - ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹

3. æµ‹è¯•ä¸è¯„ä¼°
   ```python
   # å•å…ƒæµ‹è¯•
   def test_model_output():
       model = YourModel()
       x = torch.randn(1, 3, 224, 224)
       output = model(x)
       assert output.shape == (1, num_classes)
   ```

## å…­ã€å¸¸è§é—®é¢˜è§£å†³
1. å†…å­˜ç®¡ç†
   - ä½¿ç”¨ `del` é‡Šæ”¾ä¸éœ€è¦çš„å˜é‡
   - å®šæœŸæ¸…ç†GPUç¼“å­˜
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å¤„ç†å¤§æ‰¹é‡

2. è°ƒè¯•æŠ€å·§
   ```python
   # æ‰“å°ä¸­é—´å±‚è¾“å‡º
   class DebugLayer(nn.Module):
       def forward(self, x):
           print(f"Shape: {x.shape}, Mean: {x.mean()}")
           return x
   ```

## å­¦ä¹ è·¯å¾„å»ºè®®
1. åŸºç¡€é¡¹ç›®
   - MNISTæ‰‹å†™æ•°å­—è¯†åˆ«
   - æƒ…æ„Ÿåˆ†æ
   - ç®€å•å›¾åƒåˆ†ç±»

2. è¿›é˜¶é¡¹ç›®
   - ç›®æ ‡æ£€æµ‹
   - æœºå™¨ç¿»è¯‘
   - å›¾åƒç”Ÿæˆ

3. é«˜çº§é¡¹ç›®
   - å¤šæ¨¡æ€å­¦ä¹ 
   - å¼ºåŒ–å­¦ä¹ 
   - è‡ªåŠ¨é©¾é©¶ 